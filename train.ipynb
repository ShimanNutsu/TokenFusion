{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from einops import rearrange, reduce, repeat\n",
    "\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\r\n"
     ]
    }
   ],
   "source": [
    "!echo $CUDA_VISIBLE_DEVICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshiman\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/code/TokenFusion/wandb/run-20221221_193419-1nsz7acv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/shiman/uncategorized/runs/1nsz7acv\" target=\"_blank\">my layer + mixing + vit</a></strong> to <a href=\"https://wandb.ai/shiman/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/shiman/uncategorized/runs/1nsz7acv?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7ff095ed6670>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init('Token Fusion', name='my layer + mixing + vit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.transformer import PatchEmbedding, Transformer\n",
    "from reformer_pytorch import Reformer\n",
    "from modules.fnet import FNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "train_dataset = torchvision.datasets.ImageFolder('/data/ILSVRC/Data/CLS-LOC/train', transforms.Compose([\n",
    "                transforms.Resize(256),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "                normalize,\n",
    "            ]))\n",
    "\n",
    "val_dataset = torchvision.datasets.ImageFolder('/data/ILSVRC/Data/CLS-LOC/val', transforms.Compose([\n",
    "                transforms.Resize(256),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "                normalize,\n",
    "            ]))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixingClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pool1 = nn.AdaptiveAvgPool1d(1)\n",
    "        self.pool2 = nn.AdaptiveAvgPool1d(1)\n",
    "        self.classifier = nn.LazyLinear(100)\n",
    "    def forward(self, x):\n",
    "        x1 = self.pool2(x)\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        x2 = self.pool1(x)\n",
    "        x1 = torch.squeeze(x1, dim=-1)\n",
    "        x2 = torch.squeeze(x2, dim=-1)\n",
    "        x = torch.concat([x1, x2], axis=1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "class LayerTokenFusion(nn.Module):\n",
    "    def __init__(self, classifier):\n",
    "        super().__init__()\n",
    "        self.resnet = timm.create_model('resnet101', pretrained=True, features_only=True)\n",
    "        for par in self.resnet.parameters():\n",
    "            par.requires_grad = False\n",
    "        in_ch = in_ch = [3, 64] + [2 ** i for i in range(8, 12)]\n",
    "\n",
    "        self.upconvs = nn.ModuleList([nn.ConvTranspose2d(in_channels=ch, out_channels=3, kernel_size=2**(i), stride=2**(i)) for i, ch in enumerate(in_ch)])\n",
    "        self.embedders = nn.ModuleList([PatchEmbedding(in_channels=3, out_channels=64, patch_size=16) for i, ch in enumerate(in_ch)])\n",
    "        self.transformers = nn.ModuleList([Transformer(depth=2, dim=64, mlp_ratio=1) for _ in range(6)])\n",
    "        self.attend = nn.ModuleList([nn.MultiheadAttention(embed_dim=64, num_heads=1, batch_first=True) for _ in range(6)])\n",
    "        \n",
    "        self.classifier  = classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.resnet(x)\n",
    "        x = self.embedders[0](x)\n",
    "        for i, v in enumerate(out):\n",
    "            v = self.upconvs[i + 1](v)\n",
    "            v = self.embedders[i + 1](v)\n",
    "\n",
    "            x = self.transformers[i](x)\n",
    "            #x = torch.concat([x, v], axis=1)\n",
    "            z, _ = self.attend[i](x, v, v)\n",
    "            x = x + z\n",
    "        x = self.transformers[-1](x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "x = torch.randn((3, 3, 224, 224)).cuda()\n",
    "model = LayerTokenFusion(classifier = MixingClassifier()).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "class BridgeBlock(nn.Module):\n",
    "    def __init__(self, single_weight=False):\n",
    "        super().__init__()\n",
    "        in_ch = in_ch = [64] + [2 ** i for i in range(8, 12)]\n",
    "        self.expand = nn.ModuleList([\n",
    "            nn.ConvTranspose2d(in_channels=in_ch[i], out_channels=3, kernel_size=2**(i+1),\n",
    "                               stride=2**(i+1)) for i in range(5)\n",
    "        ])\n",
    "    def forward(self, x, feature, n_layer):\n",
    "        out = self.expand[n_layer](feature)\n",
    "        x = torch.concat([x, out], axis=1)\n",
    "        return x\n",
    "\n",
    "class EarlyTokenFusion(nn.Module):\n",
    "    def __init__(self, classifier):\n",
    "        super().__init__()\n",
    "        self.resnet = timm.create_model('resnet101', features_only=True, pretrained=True)\n",
    "        for par in self.resnet.parameters():\n",
    "            par.requires_grad = False\n",
    "        self.bridge = BridgeBlock()\n",
    "        self.patch_ebmedding = PatchEmbedding(in_channels=18, out_channels=256, patch_size=16)\n",
    "        self.transformer = Transformer(depth=12, dim=256, mlp_ratio=1)\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.resnet(x)\n",
    "        for i, v in enumerate(out):\n",
    "            x = self.bridge(x, v, i)\n",
    "        x = self.patch_ebmedding(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "x = torch.randn((3, 3, 224, 224)).cuda()\n",
    "model = EarlyTokenFusion(classifier=MixingClassifier()).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "class LateTokenFusion(nn.Module):\n",
    "    def __init__(self, classifier):\n",
    "        super().__init__()\n",
    "        self.classifier = classifier\n",
    "        self.resnet = timm.create_model('resnet101', features_only=True, pretrained=True)\n",
    "        for par in self.resnet.parameters():\n",
    "            par.requires_grad = False\n",
    "\n",
    "        self.embedder1 = PatchEmbedding(in_channels=3, out_channels=256, patch_size=16)\n",
    "        self.transformer1 = Transformer(depth=8, dim=256)\n",
    "\n",
    "        self.embedder2 = PatchEmbedding(in_channels=1024, out_channels=1024, patch_size=1, same=True)\n",
    "        self.transformer2 = Transformer(depth=4, dim=1024)\n",
    "\n",
    "        self.UpConv = nn.ConvTranspose2d(in_channels=1, out_channels=1, kernel_size=(1, 4), stride=(1, 4))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x1 = self.embedder1(x)\n",
    "        x1 = self.transformer1(x1)\n",
    "        x1 = torch.unsqueeze(x1, dim=1)\n",
    "        x1 = self.UpConv(x1)\n",
    "        x1 = torch.squeeze(x1)\n",
    "\n",
    "        x2 = self.resnet(x)[-2]\n",
    "        x2 = self.embedder2(x2)\n",
    "        x2 = self.transformer2(x2)\n",
    "        \n",
    "\n",
    "        x = torch.concat([x1, x2], axis=1)\n",
    "        #x = torch.transpose(x, 1, 2)\n",
    "        #print(nn.AvgPool1d(392)(x).size())\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "class Squeeze(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.squeeze(x)\n",
    "    \n",
    "x = torch.randn((3, 3, 224, 224)).cuda()\n",
    "class MixingClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pool1 = nn.AdaptiveAvgPool1d(1)\n",
    "        self.pool2 = nn.AdaptiveAvgPool1d(1)\n",
    "        self.classifier = nn.LazyLinear(100)\n",
    "    def forward(self, x):\n",
    "        x1 = self.pool2(x)\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        x2 = self.pool1(x)\n",
    "        x1 = torch.squeeze(x1)\n",
    "        x2 = torch.squeeze(x2)\n",
    "        x = torch.concat([x1, x2], axis=1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "        \n",
    "model = LateTokenFusion(classifier = MixingClassifier()).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 100])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, optimizer, loss_fn, train_dl, val_dl, epochs=100, device='cpu'):\n",
    "\n",
    "    print('train() called: model=%s, opt=%s(lr=%f), epochs=%d, device=%s\\n' % \\\n",
    "          (type(model).__name__, type(optimizer).__name__,\n",
    "           optimizer.param_groups[0]['lr'], epochs, device))\n",
    "\n",
    "    history = {} # Collects per-epoch loss and acc like Keras' fit().\n",
    "    history['loss'] = []\n",
    "    history['val_loss'] = []\n",
    "    history['acc'] = []\n",
    "    history['val_acc'] = []\n",
    "\n",
    "    start_time_sec = time.time()\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "\n",
    "        # --- TRAIN AND EVALUATE ON TRAINING SET -----------------------------\n",
    "        model.train()\n",
    "        train_loss         = 0.0\n",
    "        num_train_correct  = 0\n",
    "        num_train_examples = 0\n",
    "\n",
    "        for batch in tqdm(train_dl):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            x    = batch[0].to(device)\n",
    "            y    = batch[1].to(device)\n",
    "            yhat = model(x)\n",
    "            loss = loss_fn(yhat, y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss         += loss.data.item() * x.size(0)\n",
    "            num_train_correct  += (torch.max(yhat, 1)[1] == y).sum().item()\n",
    "            num_train_examples += x.shape[0]\n",
    "\n",
    "        train_acc   = num_train_correct / num_train_examples\n",
    "        train_loss  = train_loss / len(train_dl.dataset)\n",
    "\n",
    "\n",
    "        # --- EVALUATE ON VALIDATION SET -------------------------------------\n",
    "        model.eval()\n",
    "        val_loss       = 0.0\n",
    "        num_val_correct  = 0\n",
    "        num_val_examples = 0\n",
    "\n",
    "        for batch in tqdm(val_dl):\n",
    "\n",
    "            x    = batch[0].to(device)\n",
    "            y    = batch[1].to(device)\n",
    "            yhat = model(x)\n",
    "            loss = loss_fn(yhat, y)\n",
    "\n",
    "            val_loss         += loss.data.item() * x.size(0)\n",
    "            num_val_correct  += (torch.max(yhat, 1)[1] == y).sum().item()\n",
    "            num_val_examples += y.shape[0]\n",
    "\n",
    "        val_acc  = num_val_correct / num_val_examples\n",
    "        val_loss = val_loss / len(val_dl.dataset)\n",
    "\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "          print('Epoch %3d/%3d, train loss: %5.2f, train acc: %5.2f, val loss: %5.2f, val acc: %5.2f' % \\\n",
    "                (epoch, epochs, train_loss, train_acc, val_loss, val_acc))\n",
    "    \n",
    "        \n",
    "        wandb.log({\n",
    "            'train loss': train_loss,\n",
    "            'val loss': val_loss,\n",
    "            'train acc': train_acc,\n",
    "            'val acc': val_acc\n",
    "        })\n",
    "        history['loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        #torch.save(model.state_dict(), 'currrr.h5')\n",
    "\n",
    "    # END OF TRAINING LOOP\n",
    "\n",
    "\n",
    "    end_time_sec       = time.time()\n",
    "    total_time_sec     = end_time_sec - start_time_sec\n",
    "    time_per_epoch_sec = total_time_sec / epochs\n",
    "    print()\n",
    "    print('Time total:     %5.2f sec' % (total_time_sec))\n",
    "    print('Time per epoch: %5.2f sec' % (time_per_epoch_sec))\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train() called: model=LayerTokenFusion, opt=AdamW(lr=0.000100), epochs=24, device=cuda\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1633/1633 [1:05:19<00:00,  2.40s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 580/580 [10:41<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/ 24, train loss:  0.11, train acc:  0.97, val loss:  0.14, val acc:  0.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                           | 1031/1633 [37:58<09:24,  1.07it/s]"
     ]
    }
   ],
   "source": [
    "history = train(model, optimizer, criterion, train_loader, val_loader, epochs=24, device='cuda')\n",
    "#torch.save(model.state_dict(), 'cur2.h5')\n",
    "#wandb.save('cur2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import torch\n",
      "import torchvision\n",
      "from torchvision import transforms\n",
      "\n",
      "import torch.nn as nn\n",
      "\n",
      "from einops import rearrange, reduce, repeat\n",
      "\n",
      "import timm\n",
      "import wandb\n",
      "\n",
      "wandb.login()\n",
      "wandb.init('Token Fusion', project='my layer + mixing + vit', resume=True)\n",
      "from modules.transformer import PatchEmbedding, Transformer\n",
      "from reformer_pytorch import Reformer\n",
      "from modules.fnet import FNet\n",
      "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "\n",
      "train_dataset = torchvision.datasets.ImageFolder('/data/ILSVRC/Data/CLS-LOC/train', transforms.Compose([\n",
      "                transforms.Resize(256),\n",
      "                transforms.CenterCrop(224),\n",
      "                transforms.ToTensor(),\n",
      "                normalize,\n",
      "            ]))\n",
      "\n",
      "val_dataset = torchvision.datasets.ImageFolder('/data/ILSVRC/Data/CLS-LOC/val', transforms.Compose([\n",
      "                transforms.Resize(256),\n",
      "                transforms.CenterCrop(224),\n",
      "                transforms.ToTensor(),\n",
      "                normalize,\n",
      "            ]))\n",
      "\n",
      "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
      "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32)\n",
      "class MixingClassifier(nn.Module):\n",
      "    def __init__(self):\n",
      "        super().__init__()\n",
      "        self.pool1 = nn.AdaptiveAvgPool1d(1)\n",
      "        self.pool2 = nn.AdaptiveAvgPool1d(1)\n",
      "        self.classifier = nn.LazyLinear(100)\n",
      "    def forward(self, x):\n",
      "        x1 = self.pool2(x)\n",
      "        x = torch.transpose(x, 1, 2)\n",
      "        x2 = self.pool1(x)\n",
      "        x1 = torch.squeeze(x1, dim=-1)\n",
      "        x2 = torch.squeeze(x2, dim=-1)\n",
      "        x = torch.concat([x1, x2], axis=1)\n",
      "        x = self.classifier(x)\n",
      "        return x\n",
      "class LayerTokenFusion(nn.Module):\n",
      "    def __init__(self, classifier):\n",
      "        super().__init__()\n",
      "        self.resnet = timm.create_model('resnet101', pretrained=True, features_only=True)\n",
      "        for par in self.resnet.parameters():\n",
      "            par.requires_grad = False\n",
      "        in_ch = in_ch = [3, 64] + [2 ** i for i in range(8, 12)]\n",
      "\n",
      "        self.upconvs = nn.ModuleList([nn.ConvTranspose2d(in_channels=ch, out_channels=3, kernel_size=2**(i), stride=2**(i)) for i, ch in enumerate(in_ch)])\n",
      "        self.embedders = nn.ModuleList([PatchEmbedding(in_channels=3, out_channels=256, patch_size=16) for i, ch in enumerate(in_ch)])\n",
      "        self.transformers = nn.ModuleList([Transformer(depth=2, dim=256, mlp_ratio=1) for _ in range(6)])\n",
      "        self.attend = nn.ModuleList([nn.MultiheadAttention(embed_dim=256, num_heads=1) for _ in range(6)])\n",
      "        \n",
      "        self.classifier  = classifier\n",
      "\n",
      "    def forward(self, x):\n",
      "        out = self.resnet(x)\n",
      "        x = self.embedders[0](x)\n",
      "        for i, v in enumerate(out):\n",
      "            v = self.upconvs[i + 1](v)\n",
      "            v = self.embedders[i + 1](v)\n",
      "\n",
      "            x = self.transformers[i](x)\n",
      "            #x = torch.concat([x, v], axis=1)\n",
      "            z, _ = self.attend[i](x, v, v)\n",
      "            x = x + z\n",
      "        x = self.transformers[-1](x)\n",
      "        x = self.classifier(x)\n",
      "        return x\n",
      "\n",
      "x = torch.randn((3, 3, 224, 224)).cuda()\n",
      "model = LayerTokenFusion(classifier = MixingClassifier()).cuda()\n",
      "model.load_state_dict(torch.load('cur.h5'))\n",
      "import time\n",
      "from tqdm import tqdm\n",
      "\n",
      "def train(model, optimizer, loss_fn, train_dl, val_dl, epochs=100, device='cpu'):\n",
      "\n",
      "    print('train() called: model=%s, opt=%s(lr=%f), epochs=%d, device=%s\\n' % \\\n",
      "          (type(model).__name__, type(optimizer).__name__,\n",
      "           optimizer.param_groups[0]['lr'], epochs, device))\n",
      "\n",
      "    history = {} # Collects per-epoch loss and acc like Keras' fit().\n",
      "    history['loss'] = []\n",
      "    history['val_loss'] = []\n",
      "    history['acc'] = []\n",
      "    history['val_acc'] = []\n",
      "\n",
      "    start_time_sec = time.time()\n",
      "\n",
      "    for epoch in range(1, epochs+1):\n",
      "\n",
      "        # --- TRAIN AND EVALUATE ON TRAINING SET -----------------------------\n",
      "        model.train()\n",
      "        train_loss         = 0.0\n",
      "        num_train_correct  = 0\n",
      "        num_train_examples = 0\n",
      "\n",
      "        for batch in tqdm(train_dl):\n",
      "\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            x    = batch[0].to(device)\n",
      "            y    = batch[1].to(device)\n",
      "            yhat = model(x)\n",
      "            loss = loss_fn(yhat, y)\n",
      "\n",
      "            loss.backward()\n",
      "            optimizer.step()\n",
      "\n",
      "            train_loss         += loss.data.item() * x.size(0)\n",
      "            num_train_correct  += (torch.max(yhat, 1)[1] == y).sum().item()\n",
      "            num_train_examples += x.shape[0]\n",
      "\n",
      "        train_acc   = num_train_correct / num_train_examples\n",
      "        train_loss  = train_loss / len(train_dl.dataset)\n",
      "\n",
      "\n",
      "        # --- EVALUATE ON VALIDATION SET -------------------------------------\n",
      "        model.eval()\n",
      "        val_loss       = 0.0\n",
      "        num_val_correct  = 0\n",
      "        num_val_examples = 0\n",
      "\n",
      "        for batch in tqdm(val_dl):\n",
      "\n",
      "            x    = batch[0].to(device)\n",
      "            y    = batch[1].to(device)\n",
      "            yhat = model(x)\n",
      "            loss = loss_fn(yhat, y)\n",
      "\n",
      "            val_loss         += loss.data.item() * x.size(0)\n",
      "            num_val_correct  += (torch.max(yhat, 1)[1] == y).sum().item()\n",
      "            num_val_examples += y.shape[0]\n",
      "\n",
      "        val_acc  = num_val_correct / num_val_examples\n",
      "        val_loss = val_loss / len(val_dl.dataset)\n",
      "\n",
      "\n",
      "        if epoch == 1 or epoch % 10 == 0:\n",
      "          print('Epoch %3d/%3d, train loss: %5.2f, train acc: %5.2f, val loss: %5.2f, val acc: %5.2f' % \\\n",
      "                (epoch, epochs, train_loss, train_acc, val_loss, val_acc))\n",
      "    \n",
      "        \n",
      "        wandb.log({\n",
      "            'train loss': train_loss,\n",
      "            'val loss': val_loss,\n",
      "            'train acc': train_acc,\n",
      "            'val acc': val_acc\n",
      "        })\n",
      "        history['loss'].append(train_loss)\n",
      "        history['val_loss'].append(val_loss)\n",
      "        history['acc'].append(train_acc)\n",
      "        history['val_acc'].append(val_acc)\n",
      "        \n",
      "        #torch.save(model.state_dict(), 'currrr.h5')\n",
      "\n",
      "    # END OF TRAINING LOOP\n",
      "\n",
      "\n",
      "    end_time_sec       = time.time()\n",
      "    total_time_sec     = end_time_sec - start_time_sec\n",
      "    time_per_epoch_sec = total_time_sec / epochs\n",
      "    print()\n",
      "    print('Time total:     %5.2f sec' % (total_time_sec))\n",
      "    print('Time per epoch: %5.2f sec' % (time_per_epoch_sec))\n",
      "\n",
      "    return history\n",
      "import torch.optim as optim\n",
      "\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.AdamW(model.parameters(), lr=0.0001)\n",
      "history = train(model, optimizer, criterion, train_loader, val_loader, epochs=16, device='cuda')\n",
      "#torch.save(model.state_dict(), 'cur2.h5')\n",
      "#wandb.save('cur2.h5')\n",
      "history\n"
     ]
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act1): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (6): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (7): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (8): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (9): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (10): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (11): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (12): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (13): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (14): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (15): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (16): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (17): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (18): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (19): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (20): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (21): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (22): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = timm.create_model('resnet101', pretrained=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model('resnet101', pretrained=True)\n",
    "\n",
    "model.eval()\n",
    "val_loss       = 0.0\n",
    "num_val_correct  = 0\n",
    "num_val_examples = 0\n",
    "\n",
    "for batch in tqdm(val_dl):\n",
    "\n",
    "    x    = batch[0].to(device)\n",
    "    y    = batch[1].to(device)\n",
    "    yhat = model(x)\n",
    "    loss = loss_fn(yhat, y)\n",
    "\n",
    "    val_loss         += loss.data.item() * x.size(0)\n",
    "    num_val_correct  += (torch.max(yhat, 1)[1] == y).sum().item()\n",
    "    num_val_examples += y.shape[0]\n",
    "\n",
    "val_acc  = num_val_correct / num_val_examples\n",
    "val_loss = val_loss / len(val_dl.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.6135925867340781,\n",
       "  0.2705157463265378,\n",
       "  0.2064774487246737,\n",
       "  0.1559920800612375,\n",
       "  0.11992709155309428,\n",
       "  0.09436373843291204,\n",
       "  0.07545991969960586,\n",
       "  0.06159499103661947,\n",
       "  0.052060843378862444,\n",
       "  0.04702815396277903,\n",
       "  0.041631026792422976,\n",
       "  0.03816739026094485,\n",
       "  0.03589167668153447,\n",
       "  0.03521862431499343,\n",
       "  0.030950547608680316,\n",
       "  0.029794174555466433],\n",
       " 'val_loss': [0.3060782493714911,\n",
       "  0.25821791502930513,\n",
       "  0.21299451223542254,\n",
       "  0.23188905120210931,\n",
       "  0.24707616758606393,\n",
       "  0.24491693581045537,\n",
       "  0.2516499959808879,\n",
       "  0.26488420082111447,\n",
       "  0.24849710881547546,\n",
       "  0.27997587498539206,\n",
       "  0.27884319014894055,\n",
       "  0.2872113626650069,\n",
       "  0.3004888705318007,\n",
       "  0.2951320726693032,\n",
       "  0.2981743334061938,\n",
       "  0.3143005586222774],\n",
       " 'acc': [0.8336937799043063,\n",
       "  0.9203253588516747,\n",
       "  0.9378564593301435,\n",
       "  0.9520765550239234,\n",
       "  0.9620478468899522,\n",
       "  0.9699521531100479,\n",
       "  0.975511961722488,\n",
       "  0.9804784688995215,\n",
       "  0.9832631578947368,\n",
       "  0.9850334928229665,\n",
       "  0.9873397129186603,\n",
       "  0.9878468899521531,\n",
       "  0.988755980861244,\n",
       "  0.9892248803827751,\n",
       "  0.9902679425837321,\n",
       "  0.9908708133971292],\n",
       " 'val_acc': [0.9116361870852889,\n",
       "  0.9277121432810056,\n",
       "  0.9387711064357771,\n",
       "  0.9382855909802018,\n",
       "  0.9368290446134757,\n",
       "  0.9421157684630739,\n",
       "  0.939850029670389,\n",
       "  0.9395263527000054,\n",
       "  0.9441117764471058,\n",
       "  0.9415223606840374,\n",
       "  0.9417920914926903,\n",
       "  0.9436262609915305,\n",
       "  0.9449209688730646,\n",
       "  0.9406592220963479,\n",
       "  0.9455143766521013,\n",
       "  0.9444354534174894]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "classes = os.listdir('/data/VasilievDV/ILSVRC/Data/CLS-LOC/train')[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "for cls in classes:\n",
    "    os.makedirs('/data/nnshiman/ILSVRC/Data/CLS-LOC/train/' + cls)\n",
    "    os.makedirs('/data/nnshiman/ILSVRC/Data/CLS-LOC/val/' + cls)\n",
    "    files = os.listdir('/data/VasilievDV/ILSVRC/Data/CLS-LOC/train/' + cls)\n",
    "    tr_files = files[:1100]\n",
    "    val_files = files[1100:]\n",
    "    for file in tr_files:\n",
    "        shutil.copyfile('/data/VasilievDV/ILSVRC/Data/CLS-LOC/train/' + cls + '/' + file,\n",
    "                       '/data/nnshiman/ILSVRC/Data/CLS-LOC/train/' + cls + '/' + file)\n",
    "    for file in val_files:\n",
    "        shutil.copyfile('/data/VasilievDV/ILSVRC/Data/CLS-LOC/train/' + cls + '/' + file,\n",
    "                       '/data/nnshiman/ILSVRC/Data/CLS-LOC/val/' + cls + '/' + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /data/nnshiman/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
